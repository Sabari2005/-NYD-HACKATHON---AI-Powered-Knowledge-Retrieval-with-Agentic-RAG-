{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain langchain-ollama ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "llamamodel = OllamaLLM(model=\"llama3.3:latest\")\n",
    "\n",
    "result = llamamodel.invoke(input=\"hello\")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers accelerate datasets sentence-transformers\n",
    "!pip install pinecone-client sentence-transformers pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to index: nyd\n"
     ]
    }
   ],
   "source": [
    "# Initialize Pinecone\n",
    "pc = Pinecone(api_key=\"pcsk_6E9B6o_DHFJaybC7zzr4QT9i1tZo1vExTxji5j1syULud17p1HXAzrZPN7Zv4fs9H83L98\")  # Replace with your Pinecone API key\n",
    "\n",
    "index_name = \"nyd\"\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=384,  # Adjust based on your embedding dimension\n",
    "        metric='cosine',  # Use cosine similarity\n",
    "        spec=ServerlessSpec(\n",
    "            cloud='aws',\n",
    "            region='us-east-1'\n",
    "        )\n",
    "    )\n",
    "index = pc.Index(index_name)\n",
    "print(f\"Connected to index: {index_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Sentence Transformer model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the dataset\n",
    "df = pd.read_csv(\"combined_file.csv\")\n",
    "df['Combined_Questions'] = df['question'].apply(lambda x: \" \".join(x.split(\"?\")).strip())\n",
    "embeddings = model.encode(df['Combined_Questions'].tolist(), convert_to_numpy=True)\n",
    "\n",
    "# Upload embeddings and metadata to Pinecone\n",
    "for i, row in df.iterrows():\n",
    "    metadata = {\n",
    "        \"answer\": row['answer'],\n",
    "        \"chapter\": row['chapter'],\n",
    "        \"verse\": row['verse'],\n",
    "        \"sanskrit\": row['sanskrit']\n",
    "    }\n",
    "    index.upsert([(str(i), embeddings[i].tolist(), metadata)])\n",
    "print(\"Embeddings with metadata uploaded to Pinecone!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to retrieve answers and metadata from Pinecone\n",
    "def retrieve_answer(input_question, top_k=1):\n",
    "    query_embedding = model.encode(input_question, convert_to_numpy=True)\n",
    "    result = index.query(\n",
    "        vector=query_embedding.tolist(),\n",
    "        top_k=top_k,\n",
    "        include_metadata=True\n",
    "    )\n",
    "    answers = []\n",
    "    for match in result['matches']:\n",
    "        metadata = match['metadata']\n",
    "        answers.append({\n",
    "            \"score\": match['score'],\n",
    "            \"answer\": metadata['answer'],\n",
    "            \"chapter\": metadata.get('chapter', 'N/A'),\n",
    "            \"verse\": metadata.get('verse', 'N/A'),\n",
    "            \"sanskrit\": metadata.get('sanskrit', 'N/A')\n",
    "        })\n",
    "    return answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_query_from_llama(query):\n",
    "    llama1result =llamamodel.invoke(input = query)\n",
    "    return llama1result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llama Query Refinement\n",
    "def refine_query_with_llama(query, retrieved_info):\n",
    "    prompt_Template_refine=f\"\"\"\"You are an assistant specializing in refining queries for better retrieval.\\n Original query: '{query}'\\n Retrieved information:\\n{retrieved_info}\\n Refine the query to include specific details for improved results.\\n If the query is already precise, return it unchanged. Refined query:\"\"\"\n",
    "    Refine_result= llamamodel.invoke(input = prompt_Template_refine)\n",
    "    return Refine_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llama Final Response Generation\n",
    "def generate_final_response_with_llama(query, retrieved_info, llm_retrieved):\n",
    "    prompt_Template_Generate = f\"\"\"You are an expert at combining information to generate detailed answers.\\n Original query: '{query}'\\n Retrieved information from semantic search:\\n{retrieved_info} \\n Retrieved information from Llama:\\n{llm_retrieved} \\n Provide chapter, verse, sanskrit, traslation if the query is directly belongs to sanskrit. \\n \"If the query is not directly related to Bhagavad Gita and Pantanjali yoga sutra return 'This Question not directly related to Bhagavad Gita or Pantanjali yoga sutra' with no extra words\"  \\n Don't say how you process this context in the answer. \\n Using all the provided context, generate a complete, accurate, and concise answer. \\n  \"Provide the output in the following dictionary format within double quotes for keys and values: \\n\\n{{\\n  Book_name' : <Book_name>,\\n  'chapter': <chapter_number>,\\n  'verse': <verse_number>,\\n  'sanskrit': <sanskrit_text>,\\n  'Translation' : <Translation>,\\n  'answer': <enlarge_answer>\\n}}\\n\\n\"\"\"\n",
    "    Generate_result = llamamodel.invoke(input =  prompt_Template_Generate)\n",
    "    return Generate_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_resp(query_test):\n",
    "  user_query =query_test\n",
    "\n",
    "\n",
    "  # Step 1: Retrieve answer from Pinecone\n",
    "  semantic_results = retrieve_answer(user_query, top_k=3)\n",
    "\n",
    "  retrieved_info = \"\\n\".join([\n",
    "      f\"Score: {item['score']}, Answer: {item['answer']}, Chapter: {item['chapter']}, Verse: {item['verse']}, Sanskrit: {item['sanskrit']}\"\n",
    "      for item in semantic_results\n",
    "  ])\n",
    "\n",
    "  # Step 2: Retrieve answer from Llama\n",
    "  llm_result = answer_query_from_llama(user_query)\n",
    "\n",
    "  # Step 3: Refine the query with Llama\n",
    "  refined_query = refine_query_with_llama(user_query, retrieved_info)\n",
    "\n",
    "  # Step 4: Generate final response using Llama\n",
    "  final_response = generate_final_response_with_llama(refined_query, retrieved_info, llm_result)\n",
    "\n",
    "  # Display the results\n",
    "  print(\"=====================================================\")\n",
    "  print(\"Semantic Search Results:\")\n",
    "  print(retrieved_info)\n",
    "  print(\"-----------------------------------------------------\")\n",
    "  print(llm_result)\n",
    "  print(\"=====================================================\")\n",
    "  print(f\"Refined Query: {refined_query}\")\n",
    "  print(\"-----------------------------------------------------\")\n",
    "  print(f\"Final Response:\\n{final_response}\")\n",
    "  print(\"=====================================================\")\n",
    "  return final_response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import os\n",
    "\n",
    "query_test = input(\"Enter the Question: \")\n",
    "# Get the response from the final_resp function \n",
    "final_dic = final_resp(query_test)\n",
    "output_folder = \"output_folder\"\n",
    "\n",
    "try:\n",
    "  # Convert the JSON string to a Python dictionary\n",
    "  parsed_dict = json.loads(final_dic)\n",
    "\n",
    "\n",
    "# Save the parsed dictionary as a JSON file\n",
    "  json_filename = f\"{output_folder}/output_.json\" \n",
    "  with open(json_filename, \"w\", encoding=\"utf-8\") as json_file:\n",
    "      json.dump(parsed_dict, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "  print(f\"Saved JSON file for this Question : {json_filename}\")\n",
    "except json.JSONDecodeError as e:\n",
    "  print(f\"Error decoding JSON for this Question : {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
